{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fee4d59f",
   "metadata": {},
   "source": [
    "## Activ4a dataset decoding\n",
    "\n",
    "The datasets of Activ4a (phs002694) were provided as CSV files that contained encoded data. For the BioData Catalyst PIC-SURE ETL pipeline, these files need to be in the decoded format. The purpose of this notebook is to use the Data Dictionary, provided in Excel format, to decode the CSV files and save them to the S3 bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc33d3a",
   "metadata": {},
   "source": [
    "### Do imports, set directory paths, and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbb7731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do imports\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3854f4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directories and get file information\n",
    "data_dict_path = \"/home/ec2-user/SageMaker/pic-sure-metadata-curation/activ4a/input/Data Dictionary 2022-01-04.xlsx\"\n",
    "file_dir = \"/home/ec2-user/studies/ALL-avillach-73-bdcatalyst-etl/activ4a/development_raw_data/*\"\n",
    "out_dir = \"/home/ec2-user/studies/ALL-avillach-73-bdcatalyst-etl/activ4a/decoded_data/\"\n",
    "file_prefixes = [i.split('/')[-1].strip(\".csv\") for i in glob.glob(file_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e804d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_data(data_dict_path, file_prefixes, file_dir, out_dir, to_pop=None):\n",
    "    for f in file_prefixes:\n",
    "        data_dict = pd.read_excel(data_dict_path, \n",
    "                      sheet_name = f, \n",
    "                      engine = \"openpyxl\")\n",
    "        data_dict = data_dict.dropna(subset=[\"code\"]).reset_index(drop=True)\n",
    "        data_dict['code'] = data_dict['code'].astype('str') \n",
    "        full_dict = make_decode_dict(data_dict, to_pop)\n",
    "        try:\n",
    "            data = pd.read_csv(file_dir.strip(\"*\")+f+\".csv\")\n",
    "        except:\n",
    "            print(\"Review data of\", f)\n",
    "            continue\n",
    "        try: \n",
    "            decoded_data = data.replace(full_dict)\n",
    "        except:\n",
    "            print(\"Review data dictionary of\", f)\n",
    "            print(\"Unsuccessful\")\n",
    "            continue\n",
    "        out_file = out_dir+f+\".csv\"\n",
    "        decoded_data.to_csv(out_file, index=False)\n",
    "        print(\"Decoded dataset\", out_file)\n",
    "        \n",
    "def make_decode_dict(df, pop_val):\n",
    "    full_dict = {}\n",
    "    for var in df.VARNAME:\n",
    "        data = df.code[df.VARNAME == var].values[0]\n",
    "        list_data = data.split(\"|\")\n",
    "        mini_dict = {}\n",
    "        for info in list_data:\n",
    "            mapping = info.split(\"=\")\n",
    "            try:\n",
    "                encoded_val = int(mapping[0])\n",
    "            except:\n",
    "                encoded_val = mapping[0].strip().strip(\"'\")\n",
    "            if (pop_val is not None and encoded_val not in pop_val) or pop_val is None:\n",
    "                decoded_val = mapping[1].strip().strip(\"'\")\n",
    "                mini_dict.update({encoded_val: decoded_val})\n",
    "        full_dict.update({var: mini_dict})\n",
    "    return full_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012b5db7",
   "metadata": {},
   "source": [
    "### Use functions to decode the data\n",
    "Note that based on the output, there were some files that we were not able to decode without some manual curation. These were:\n",
    "- ADJ_STROKE\n",
    "- BIOREP\n",
    "- CMBL\n",
    "- CONS\n",
    "- HS\n",
    "- IE\n",
    "- LBRES\n",
    "- OUTCOME\n",
    "- REHOSP\n",
    "- SAMPLE\n",
    "- SD\n",
    "- STATUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8981fbbd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "decode_data(data_dict_path, file_prefixes, file_dir, out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5473f62",
   "metadata": {},
   "source": [
    "### Manual review of problematic data dictionaries\n",
    "After reviewing the data dictionaries and data files by hand, there were some decoded values that were not in the data and thus causing problems. These included `.U`, `.A`, and `.D`. Since these are strings while the type of the data were numeric, this was causing problems in the `replace` function. Therefore, the functions were adjusted to include an option to exclude these parts of the data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe174f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to review data dictionary ADJ_STROKE (pop .U), BIOREP (pop .A), CONS (pop .U and .D), HS (pop .U), \n",
    "# IE (pop .A), LBRES (pop .D), SAMPLE (pop .A), SD (pop .D), STATUS (pop .U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5813113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pop_prefixes = ['ADJ_STROKE', 'HS', 'STATUS', \"MH\", \"OUTCOME\"]\n",
    "a_pop_prefixes = ['BIOREP', 'IE', 'SAMPLE']\n",
    "d_pop_prefixes = ['LBRES',  'SD']\n",
    "ud_pop_prefixes = ['CONS']\n",
    "aud_pop_prefixes = [\"AE\"]\n",
    "ad_pop_prefixes = [\"VS\"]\n",
    "decode_data(data_dict_path, u_pop_prefixes, file_dir, out_dir, to_pop=['.U'])\n",
    "decode_data(data_dict_path, a_pop_prefixes, file_dir, out_dir, to_pop=['.A'])\n",
    "decode_data(data_dict_path, d_pop_prefixes, file_dir, out_dir, to_pop=['.D'])\n",
    "decode_data(data_dict_path, ud_pop_prefixes, file_dir, out_dir, to_pop=['.U', '.D'])\n",
    "decode_data(data_dict_path, aud_pop_prefixes, file_dir, out_dir, to_pop=['.A', '.U', '.D'])\n",
    "decode_data(data_dict_path, ad_pop_prefixes, file_dir, out_dir, to_pop=['.A', '.D'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9a4296",
   "metadata": {},
   "source": [
    "### Manual review of problematic data\n",
    "After attempting to review the data files on the S3 bucket through Service Workbench, it became obvious that the issue with some of these data files were the encoding. This was resolved by manually saving these files in the \"CSV UTF-8\" format and reuploading them to the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79021eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_data(data_dict_path, [\"CMBL\"], file_dir, out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158bee86",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_data(data_dict_path, [\"OUTCOME\"], file_dir, out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486db680",
   "metadata": {},
   "source": [
    "### Confirm that data was decoded as expected\n",
    "Since there were some unique problems with some of the data files and dictionaries, compare the raw and decoded CSV files to see if they changed.\n",
    "The following files need further investigation:\n",
    "- AE\n",
    "- MH\n",
    "- MPRCT\n",
    "- OUTCOME\n",
    "- VS\n",
    "- WITHDSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37946dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_decode(file_dir, out_dir, file_prefixes, out=False):\n",
    "    for f in file_prefixes:\n",
    "        raw_df = pd.read_csv(file_dir.strip(\"*\")+f+\".csv\")\n",
    "        decode_df = pd.read_csv(out_dir+f+\".csv\")\n",
    "        comp = decode_df.compare(raw_df)\n",
    "        if comp.shape[0] == 0:\n",
    "            print(f, \"not decoded properly\")\n",
    "        else:\n",
    "            print(\"Successful\", f)\n",
    "        if out:\n",
    "            return raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2cfe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_decode(file_dir, out_dir, file_prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8788c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually check each file\n",
    "#ae = check_decode(file_dir, out_dir, [\"AE\"], True)\n",
    "#cmbl = check_decode(file_dir, out_dir, ['CMBL'], True)\n",
    "#mh = check_decode(file_dir, out_dir, ['MH'], True)\n",
    "#mprct = check_decode(file_dir, out_dir, ['MPRCT'], True)\n",
    "#outcome = check_decode(file_dir, out_dir, ['OUTCOME'], True)\n",
    "#rehosp = check_decode(file_dir, out_dir, ['REHOSP'], True)\n",
    "#vs = check_decode(file_dir, out_dir, ['VS'], True)\n",
    "#withdsc = check_decode(file_dir, out_dir, ['WITHDSC'], True)\n",
    "#withdsc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c2dc21",
   "metadata": {},
   "source": [
    "### Results from check:\n",
    "- Need to fix cmbl, outcome, rehosp\n",
    "- AE, MH, VS data dictionary and data file do not match\n",
    "    - Note: there were two files hosted on Gen3 for these specific files. This was fixed by uploading the other version of the file to the S3 bucket and rerunning the code.\n",
    "- MPRCT there is no data to decode, all variables are strings or numeric. This file is OK.\n",
    "- WITHDSC is an empty dataframe, therefore no data to decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ea107b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual check of complicated files\n",
    "#f = 'OUTCOME'\n",
    "#to_pop = None\n",
    "#data_dict = pd.read_excel(data_dict_path, sheet_name = f, engine = \"openpyxl\")\n",
    "#data_dict = data_dict.dropna(subset=[\"code\"]).reset_index(drop=True)\n",
    "#data_dict['code'] = data_dict['code'].astype('str') \n",
    "#full_dict = make_decode_dict(data_dict, to_pop)\n",
    "#full_dict = {}\n",
    "#for var in data_dict.VARNAME:\n",
    "#    data = data_dict.code[data_dict.VARNAME == var].values[0]\n",
    "#    list_data = data.split(\"|\")\n",
    "#    print(list_data)\n",
    "#    mini_dict = {}\n",
    "#    #for info in list_data:\n",
    "#    #    mapping = info.split(\"=\")\n",
    "#    #    try:\n",
    "#    #        encoded_val = int(mapping[0])\n",
    "#    #    except:\n",
    "#    #        encoded_val = mapping[0].strip().strip(\"'\")\n",
    "#    #    if (to_pop is not None and encoded_val not in to_pop) or to_pop is None:\n",
    "#    #        decoded_val = mapping[1].strip().strip(\"'\")\n",
    "#    #        mini_dict.update({encoded_val: decoded_val})\n",
    "#    ##print(encoded_val, decoded_val)\n",
    "#    #full_dict.update({var: mini_dict})\n",
    "#print(full_dict)\n",
    "#try:\n",
    "#    data = pd.read_csv(file_dir.strip(\"*\")+f+\".csv\")\n",
    "#except:\n",
    "#    print(\"Review data of\", f)\n",
    "#try: \n",
    "#    decoded_data = data.replace(full_dict)\n",
    "#except:\n",
    "#    print(\"Review data dictionary of\", f)\n",
    "#    print(\"Unsuccessful\")\n",
    "#out_file = out_dir+f+\".csv\"\n",
    "#decoded_data.to_csv(out_file, index=False)\n",
    "#print(\"Decoded dataset\", out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca70d605",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
